{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythia-2.8B Baseline & KVPress 压缩实验\n",
    "\n",
    "本 Notebook 旨在复现与评估无训练 KV Cache 压缩方法在 **Pythia-2.8B** 模型上的效果。\n",
    "\n",
    "**实验设置：**\n",
    "*   **模型**: `EleutherAI/pythia-2.8B`\n",
    "*   **方法**: Baseline (无压缩), StreamingLLM\n",
    "*   **数据集**: \n",
    "    *   `wikitext-2`: 标准 PPL 测试\n",
    "    *   `pg-19`: 超长文本测试 (取单一 sample)\n",
    "*   **指标**: Perplexity (PPL) 和 推理加速比 (Speedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\35000\\Code_repository\\NLP_env\\CS3602-NLP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# 设置设备\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pythia的输入输出\n",
    "首先我们使用70m模型了解pythia的格式（即GPTNeoXAttention）\n",
    "\n",
    "- GPTNeoX 的forward函数显式接受 layer_past / cache_position / position_embeddings ，内部用 Cache 维护 KV。\n",
    "- Pythia 使用的是 DynamicCache + DynamicLayer 。\n",
    "- 每层 KV 的形状是 [batch, num_heads, seq_len, head_dim] 。\n",
    "- DynamicLayer.keys / values 是可读可写的张量属性（kvpress 里也就是这样直接赋值）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, hidden_states: torch.FloatTensor, attention_mask: torch.FloatTensor, head_mask: Optional[torch.FloatTensor] = None, layer_past: Optional[transformers.cache_utils.Cache] = None, output_attentions: Optional[bool] = False, cache_position: Optional[torch.LongTensor] = None, position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None, **kwargs: Unpack[transformers.modeling_flash_attention_utils.FlashAttentionKwargs])\n",
      "loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: cfbc5b79-3396-4c46-b991-3eac81e0b2be)')' thrown while requesting HEAD https://huggingface.co/EleutherAI/pythia-70m/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache type: <class 'transformers.cache_utils.DynamicCache'>\n",
      "num layers: 6\n",
      "layer0 type: <class 'transformers.cache_utils.DynamicLayer'>\n",
      "keys shape: torch.Size([1, 8, 121, 64])\n",
      "values shape: torch.Size([1, 8, 121, 64])\n",
      "dir(layer0): ['batch_repeat_interleave', 'batch_select_indices', 'crop', 'device', 'dtype', 'get_mask_sizes', 'get_max_cache_shape', 'get_seq_length', 'is_compileable', 'is_initialized', 'is_sliding', 'keys', 'lazy_initialization', 'offload', 'prefetch', 'reorder_cache', 'reset', 'update', 'values']\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.gpt_neox.modeling_gpt_neox import GPTNeoXAttention\n",
    "import inspect\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import Cache\n",
    "\n",
    "print(inspect.signature(GPTNeoXAttention.forward))\n",
    "\n",
    "\n",
    "model_id = \"EleutherAI/pythia-70m\"\n",
    "print(\"loading model...\")\n",
    "model_test = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "model_test.eval()\n",
    "\n",
    "text = \"Hello world, this is a streaming KV cache test. \" * 10\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "inputs = tok(text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    out = model_test(**inputs, use_cache=True)\n",
    "\n",
    "cache = out.past_key_values\n",
    "print(\"cache type:\", type(cache))\n",
    "print(\"num layers:\", len(cache.layers))\n",
    "layer0 = cache.layers[0]\n",
    "print(\"layer0 type:\", type(layer0))\n",
    "print(\"keys shape:\", layer0.keys.shape)\n",
    "print(\"values shape:\", layer0.values.shape)\n",
    "print(\"dir(layer0):\", [a for a in dir(layer0) if not a.startswith(\"_\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将StreamingLLM适配于pythia模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingLLM:\n",
    "    def __init__(self, n_sink: int = 4, window_size: int = 256):\n",
    "        self.n_sink = n_sink\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def build_context(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        if input_ids.size(1) <= self.n_sink + self.window_size:\n",
    "            return input_ids\n",
    "        sink = input_ids[:, : self.n_sink]\n",
    "        tail = input_ids[:, -self.window_size :]\n",
    "        return torch.cat([sink, tail], dim=1)\n",
    "\n",
    "    def compress_cache(self, cache) -> None:\n",
    "        for layer in cache.layers:\n",
    "            keys = layer.keys\n",
    "            values = layer.values\n",
    "            bsz, num_heads, seq_len, head_dim = keys.shape\n",
    "            if seq_len <= self.n_sink + self.window_size:\n",
    "                continue\n",
    "            device = keys.device\n",
    "            sink_end = min(self.n_sink, seq_len)\n",
    "            tail_len = min(self.window_size, seq_len - sink_end)\n",
    "            if tail_len <= 0:\n",
    "                keep_idx = torch.arange(0, sink_end, device=device)\n",
    "            else:\n",
    "                tail_start = seq_len - tail_len\n",
    "                keep_prefix = torch.arange(0, sink_end, device=device)\n",
    "                keep_tail = torch.arange(tail_start, seq_len, device=device)\n",
    "                keep_idx = torch.cat([keep_prefix, keep_tail], dim=0)\n",
    "            keys = keys.index_select(2, keep_idx)\n",
    "            values = values.index_select(2, keep_idx)\n",
    "            layer.keys = keys\n",
    "            layer.values = values\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        model: AutoModelForCausalLM,\n",
    "        input_ids: torch.Tensor,\n",
    "        max_new_tokens: int = 50,\n",
    "        temperature: float | None = None,\n",
    "        top_k: int | None = None,\n",
    "        top_p: float | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        device = next(model.parameters()).device\n",
    "        input_ids = input_ids.to(device)\n",
    "        outputs = model(input_ids, use_cache=True)\n",
    "        cache = outputs.past_key_values\n",
    "        self.compress_cache(cache)\n",
    "        generated = input_ids\n",
    "        last_token = generated[:, -1:]\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = model(last_token, use_cache=True, past_key_values=cache)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            cache = outputs.past_key_values\n",
    "            self.compress_cache(cache)\n",
    "            if temperature is not None and temperature > 0:\n",
    "                logits = logits / temperature\n",
    "            if top_k is not None and top_k > 0:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                min_values = v[:, -1].unsqueeze(-1)\n",
    "                logits = torch.where(logits < min_values, torch.full_like(logits, -float(\"inf\")), logits)\n",
    "            if top_p is not None and 0 < top_p < 1:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "                cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                logits = logits.masked_fill(indices_to_remove, -float(\"inf\"))\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "            last_token = next_token\n",
    "        return generated\n",
    "\n",
    "def load_model_and_tokenizer(model_id: str, torch_dtype=torch.float16):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch_dtype,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    return model, tokenizer\n",
    "\n",
    "def streaming_generate_from_text(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    prompt: str,\n",
    "    n_sink: int = 4,\n",
    "    window_size: int = 256,\n",
    "    max_new_tokens: int = 50,\n",
    ") -> str:\n",
    "    wrapper = StreamingLLM(n_sink=n_sink, window_size=window_size)\n",
    "    encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids\n",
    "    generated_ids = wrapper.generate(model, input_ids, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_speed_streaming_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    n_sink: int = 4,\n",
    "    window_size: int = 256,\n",
    "    num_tokens: int = 50,\n",
    "    batch_size: int = 1,\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = enc.input_ids.to(device)\n",
    "    input_ids = input_ids.repeat(batch_size, 1)\n",
    "\n",
    "    stream = StreamingLLM(n_sink=n_sink, window_size=window_size)\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, use_cache=True)\n",
    "    cache = outputs.past_key_values\n",
    "    stream.compress_cache(cache)\n",
    "    generated = input_ids\n",
    "    last_token = generated[:, -1:]\n",
    "\n",
    "    ttft = None\n",
    "\n",
    "    for step in range(num_tokens):\n",
    "        step_start = time.time() if step == 0 else None\n",
    "        with torch.no_grad():\n",
    "            outputs = model(last_token, use_cache=True, past_key_values=cache)\n",
    "        cache = outputs.past_key_values\n",
    "        stream.compress_cache(cache)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "        last_token = next_token\n",
    "        if step == 0:\n",
    "            ttft = time.time() - start\n",
    "\n",
    "    end = time.time()\n",
    "    total_time = end - start\n",
    "    if ttft is None:\n",
    "        ttft = total_time\n",
    "\n",
    "    token_phase_time = max(total_time - ttft, 1e-6)\n",
    "    total_tokens = num_tokens * batch_size\n",
    "    tpot = token_phase_time / total_tokens\n",
    "    throughput = total_tokens / token_phase_time\n",
    "\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    prompt_len = input_ids.shape[1]\n",
    "    total_tokens_processed = (prompt_len + num_tokens) * batch_size\n",
    "    total_flops = 2.0 * num_params * total_tokens_processed\n",
    "    avg_flops_per_token = total_flops / total_tokens_processed\n",
    "\n",
    "    return {\n",
    "        \"ttft\": ttft,\n",
    "        \"tpot\": tpot,\n",
    "        \"throughput\": throughput,\n",
    "        \"total_time\": total_time,\n",
    "        \"total_flops\": total_flops,\n",
    "        \"avg_flops_per_token\": avg_flops_per_token,\n",
    "    }\n",
    "\n",
    "\n",
    "def benchmark_speed_baseline(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    num_tokens: int = 50,\n",
    "    batch_size: int = 1,\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = enc.input_ids.to(device)\n",
    "    input_ids = input_ids.repeat(batch_size, 1)\n",
    "\n",
    "    start = time.time()\n",
    "    ttft = None\n",
    "\n",
    "    generated = input_ids\n",
    "    cache = None\n",
    "\n",
    "    for step in range(num_tokens):\n",
    "        with torch.no_grad():\n",
    "            if cache is None:\n",
    "                outputs = model(generated, use_cache=True)\n",
    "            else:\n",
    "                outputs = model(generated[:, -1:], use_cache=True, past_key_values=cache)\n",
    "        cache = outputs.past_key_values\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "        if step == 0:\n",
    "            ttft = time.time() - start\n",
    "\n",
    "    end = time.time()\n",
    "    total_time = end - start\n",
    "    if ttft is None:\n",
    "        ttft = total_time\n",
    "\n",
    "    token_phase_time = max(total_time - ttft, 1e-6)\n",
    "    total_tokens = num_tokens * batch_size\n",
    "    tpot = token_phase_time / total_tokens\n",
    "    throughput = total_tokens / token_phase_time\n",
    "\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    prompt_len = input_ids.shape[1]\n",
    "    total_tokens_processed = (prompt_len + num_tokens) * batch_size\n",
    "    total_flops = 2.0 * num_params * total_tokens_processed\n",
    "    avg_flops_per_token = total_flops / total_tokens_processed\n",
    "\n",
    "    return {\n",
    "        \"ttft\": ttft,\n",
    "        \"tpot\": tpot,\n",
    "        \"throughput\": throughput,\n",
    "        \"total_time\": total_time,\n",
    "        \"total_flops\": total_flops,\n",
    "        \"avg_flops_per_token\": avg_flops_per_token,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_ppl_streaming_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text: str,\n",
    "    n_sink: int = 4,\n",
    "    window_size: int = 256,\n",
    "    max_tokens: int = 2000,\n",
    "):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = enc.input_ids[:, :max_tokens].to(next(model.parameters()).device)\n",
    "    seq_len = input_ids.size(1)\n",
    "    if seq_len < 2:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    stream = StreamingLLM(n_sink=n_sink, window_size=window_size)\n",
    "    cache = None\n",
    "    losses = []\n",
    "    ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for pos in range(seq_len - 1):\n",
    "        cur = input_ids[:, pos:pos + 1]\n",
    "        with torch.no_grad():\n",
    "            if cache is None:\n",
    "                outputs = model(cur, use_cache=True)\n",
    "            else:\n",
    "                outputs = model(cur, use_cache=True, past_key_values=cache)\n",
    "            cache = outputs.past_key_values\n",
    "            stream.compress_cache(cache)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            target = input_ids[:, pos + 1]\n",
    "            loss = ce(logits, target).mean()\n",
    "            losses.append(loss)\n",
    "\n",
    "    ppl = torch.exp(torch.stack(losses).mean())\n",
    "    return ppl.item()\n",
    "\n",
    "\n",
    "def evaluate_ppl_baseline(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text: str,\n",
    "    max_tokens: int = 2000,\n",
    "):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = enc.input_ids[:, :max_tokens].to(next(model.parameters()).device)\n",
    "    seq_len = input_ids.size(1)\n",
    "    if seq_len < 2:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    max_length = model.config.max_position_embeddings\n",
    "    stride = 512\n",
    "    nlls = []\n",
    "    prev_end = 0\n",
    "    ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for begin in range(0, seq_len, stride):\n",
    "        end = min(begin + max_length, seq_len)\n",
    "        trg_len = end - prev_end\n",
    "        if trg_len <= 0:\n",
    "            break\n",
    "        cur = input_ids[:, begin:end]\n",
    "        target = cur.clone()\n",
    "        target[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(cur, labels=target)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        nlls.append(loss)\n",
    "        prev_end = end\n",
    "        if end == seq_len:\n",
    "            break\n",
    "\n",
    "    if not nlls:\n",
    "        return float(\"inf\")\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    return ppl.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"EleutherAI/pythia-2.8b\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki text length: 35786\n",
      "PG-19 text length: 100000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_long_text_from_dataset(\n",
    "    dataset_name: str = \"wikitext\",\n",
    "    split: str = \"test\",\n",
    "    limit_samples: int = 1,\n",
    "    max_chars: int | None = None,\n",
    ") -> str:\n",
    "    if dataset_name == \"wikitext\":\n",
    "        ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=split)\n",
    "        \n",
    "        # 正确获取文本的方法：\n",
    "        # 方法1：使用列表推导\n",
    "        texts = [item[\"text\"] for item in ds.select(range(limit_samples))]\n",
    "        # 或者方法2：直接切片\n",
    "        # texts = [ds[i][\"text\"] for i in range(min(limit_samples, len(ds)))]\n",
    "        \n",
    "        text = \"\\n\\n\".join(texts)\n",
    "    \n",
    "    elif dataset_name == \"pg19\":\n",
    "        ds = load_dataset(\"pg19\", split=split, streaming=True)\n",
    "        sample = next(iter(ds))\n",
    "        text = sample[\"text\"]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "    if max_chars is not None and len(text) > max_chars:\n",
    "        text = text[:max_chars]\n",
    "    \n",
    "    return text\n",
    "# 固定一次抽样\n",
    "text_wiki = load_long_text_from_dataset(\n",
    "    dataset_name=\"wikitext\",\n",
    "    split=\"train\",\n",
    "    limit_samples=100,\n",
    "    max_chars=100000,\n",
    ")\n",
    "\n",
    "text_pg19 = load_long_text_from_dataset(\n",
    "    dataset_name=\"pg19\",\n",
    "    split=\"test\",\n",
    "    limit_samples=1,\n",
    "    max_chars=100000,\n",
    ")\n",
    "\n",
    "print(\"Wiki text length:\", len(text_wiki))\n",
    "print(\"PG-19 text length:\", len(text_pg19))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验主循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamingLLM sample:\n",
      " '在一座海边小城里，工程师正在测试一种新的 KV 缓存压缩算法。そこ他们通过维护一个 KV 环形表（key-value table）来结合在一起，并乘以预期的数据量来评估响应'\n",
      "Baseline PPL on custom_complex_dataset: 11.485358238220215\n",
      "StreamingLLM PPL(256) on custom_complex_dataset: 247.625\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "prompt = \"在一座海边小城里，工程师正在测试一种新的 KV 缓存压缩算法。\"\n",
    "\n",
    "# 生成效果看看（Streaming）\n",
    "encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = encoded.input_ids\n",
    "stream = StreamingLLM(n_sink=4, window_size=256)\n",
    "out_ids = stream.generate(model, input_ids, max_new_tokens=50)\n",
    "print(\"StreamingLLM sample:\\n\", repr(tokenizer.decode(out_ids[0], skip_special_tokens=False)))\n",
    "\n",
    "with open(\"text/long_normal.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 1) Baseline PPL\n",
    "ppl_base = evaluate_ppl_baseline(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "print(\"Baseline PPL on custom_complex_dataset:\", ppl_base)\n",
    "\n",
    "# 2) StreamingLLM PPL\n",
    "ppl_stream = evaluate_ppl_streaming_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text,\n",
    "    n_sink=4,\n",
    "    window_size=256,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "print(\"StreamingLLM PPL(256) on custom_complex_dataset:\", ppl_stream)\n",
    "\n",
    "# 2) StreamingLLM PPL\n",
    "ppl_stream1 = evaluate_ppl_streaming_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text,\n",
    "    n_sink=4,\n",
    "    window_size=512,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "print(\"StreamingLLM PPL(512) on custom_complex_dataset:\", ppl_stream1)\n",
    "\n",
    "# 3) Baseline Speed\n",
    "speed_base = benchmark_speed_baseline(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    num_tokens=50,\n",
    "    batch_size=1,\n",
    ")\n",
    "print(\"Baseline speed (tok/s):\", speed_base)\n",
    "\n",
    "# 4) StreamingLLM Speed\n",
    "speed_stream = benchmark_speed_streaming_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    n_sink=4,\n",
    "    window_size=256,\n",
    "    num_tokens=50,\n",
    "    batch_size=1,\n",
    ")\n",
    "print(\"StreamingLLM(256) KV-level speed (tok/s):\", speed_stream)\n",
    "\n",
    "speed_stream1 = benchmark_speed_streaming_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    n_sink=4,\n",
    "    window_size=512,\n",
    "    num_tokens=50,\n",
    "    batch_size=1,\n",
    ")\n",
    "print(\"StreamingLLM(512) KV-level speed (tok/s):\", speed_stream1)\n",
    "\n",
    "# 简单汇总\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Baseline   - PPL: {ppl_base:.3f},  Speed: {speed_base:.2f} tok/s\")\n",
    "print(f\"Streaming(256)  - PPL: {ppl_stream:.3f},  Speed: {speed_stream:.2f} tok/s\")\n",
    "print(f\"Streaming(512)  - PPL: {ppl_stream1:.3f},  Speed: {speed_stream1:.2f} tok/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki/pg19 文本评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline PPL on WikiText (1000 tokens): 13.31332778930664\n",
      "StreamingLLM(256) PPL on WikiText (1000 tokens): 150.125\n",
      "StreamingLLM(512) PPL on WikiText (1000 tokens): 13.3046875\n"
     ]
    }
   ],
   "source": [
    "# PPL测试\n",
    "text_wiki_ppl = text_wiki[:5000]  \n",
    "ppl_base_wiki = evaluate_ppl_baseline(model, tokenizer, text_wiki_ppl, max_tokens=1000)\n",
    "print(\"Baseline PPL on WikiText (1000 tokens):\", ppl_base_wiki)\n",
    "\n",
    "ppl_stream_wiki_256 = evaluate_ppl_streaming_kv(model, tokenizer, text_wiki_ppl, n_sink=8, window_size=256, max_tokens=1000)\n",
    "print(\"StreamingLLM(256) PPL on WikiText (1000 tokens):\", ppl_stream_wiki_256)\n",
    "\n",
    "ppl_stream_wiki_512 = evaluate_ppl_streaming_kv(model, tokenizer, text_wiki_ppl, n_sink=8, window_size=1024, max_tokens=1000)\n",
    "print(\"StreamingLLM(512) PPL on WikiText (1000 tokens):\", ppl_stream_wiki_512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline speed on WikiText (500+1000 tokens): {'ttft': 0.5776915550231934, 'tpot': 0.047968844890594484, 'throughput': 20.846864298708088, 'total_time': 48.546536445617676, 'total_flops': 8114710999040.0, 'avg_flops_per_token': 5550417920.0}\n",
      "StreamingLLM(256) speed on WikiText (500+1000 tokens): {'ttft': 0.28908491134643555, 'tpot': 0.08570580220222473, 'throughput': 11.667821481216382, 'total_time': 85.99488711357117, 'total_flops': 8114710999040.0, 'avg_flops_per_token': 5550417920.0}\n",
      "StreamingLLM(512) speed on WikiText (500+1000 tokens): {'ttft': 0.2142009735107422, 'tpot': 0.059623798847198484, 'throughput': 16.771826340061967, 'total_time': 59.83799982070923, 'total_flops': 8114710999040.0, 'avg_flops_per_token': 5550417920.0}\n"
     ]
    }
   ],
   "source": [
    "# 加速测试\n",
    "prompt_wiki = text_wiki[:2000]\n",
    "speed_base_wiki = benchmark_speed_baseline(model, tokenizer, prompt_wiki, num_tokens=1000, batch_size=1)\n",
    "print(\"Baseline speed on WikiText (500+1000 tokens):\", speed_base_wiki)\n",
    "\n",
    "speed_stream_wiki_256 = benchmark_speed_streaming_kv(model, tokenizer, prompt_wiki, n_sink=8, window_size=256, num_tokens=1000, batch_size=1)\n",
    "print(\"StreamingLLM(256) speed on WikiText (500+1000 tokens):\", speed_stream_wiki_256)\n",
    "\n",
    "speed_stream_wiki_512 = benchmark_speed_streaming_kv(model, tokenizer, prompt_wiki, n_sink=8, window_size=512, num_tokens=1000, batch_size=1)\n",
    "print(\"StreamingLLM(512) speed on WikiText (500+1000 tokens):\", speed_stream_wiki_512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline PPL on PG-19 (1000 tokens): 8.540501594543457\n",
      "StreamingLLM(256) PPL on PG-19 (1000 tokens): 78.5\n",
      "StreamingLLM(512) PPL on PG-19 (1000 tokens): 35.125\n"
     ]
    }
   ],
   "source": [
    "# PPL测试  \n",
    "text_pg19_ppl = text_pg19[:5000]  # 2048 tokens\n",
    "ppl_base_pg19 = evaluate_ppl_baseline(model, tokenizer, text_pg19_ppl, max_tokens=1000)\n",
    "print(\"Baseline PPL on PG-19 (1000 tokens):\", ppl_base_pg19)\n",
    "\n",
    "ppl_stream_pg19_256 = evaluate_ppl_streaming_kv(model, tokenizer, text_pg19_ppl, n_sink=8, window_size=256, max_tokens=1000)\n",
    "print(\"StreamingLLM(256) PPL on PG-19 (1000 tokens):\", ppl_stream_pg19_256)\n",
    "\n",
    "ppl_stream_pg19_512 = evaluate_ppl_streaming_kv(model, tokenizer, text_pg19_ppl, n_sink=8, window_size=512, max_tokens=1000)\n",
    "print(\"StreamingLLM(512) PPL on PG-19 (1000 tokens):\", ppl_stream_pg19_512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline speed on PG-19 (500+1000 tokens): {'ttft': 0.4057044982910156, 'tpot': 0.05524669170379639, 'throughput': 18.100631352940958, 'total_time': 55.6523962020874, 'total_flops': 8397782312960.0, 'avg_flops_per_token': 5550417920.0}\n",
      "StreamingLLM(256) speed on PG-19 (500+1000 tokens): {'ttft': 0.3122568130493164, 'tpot': 0.08692020392417908, 'throughput': 11.504805037874796, 'total_time': 87.2324607372284, 'total_flops': 8397782312960.0, 'avg_flops_per_token': 5550417920.0}\n",
      "StreamingLLM(512) speed on PG-19 (500+1000 tokens): {'ttft': 0.2873044013977051, 'tpot': 0.0594079430103302, 'throughput': 16.832765945559068, 'total_time': 59.695247411727905, 'total_flops': 8397782312960.0, 'avg_flops_per_token': 5550417920.0}\n"
     ]
    }
   ],
   "source": [
    "# 加速测试\n",
    "prompt_pg19 = text_pg19[:2000]\n",
    "speed_base_pg19 = benchmark_speed_baseline(model, tokenizer, prompt_pg19, num_tokens=1000, batch_size=1)\n",
    "print(\"Baseline speed on PG-19 (500+1000 tokens):\", speed_base_pg19)\n",
    "\n",
    "speed_stream_pg19_256 = benchmark_speed_streaming_kv(model, tokenizer, prompt_pg19, n_sink=8, window_size=256, num_tokens=1000, batch_size=1)\n",
    "print(\"StreamingLLM(256) speed on PG-19 (500+1000 tokens):\", speed_stream_pg19_256)\n",
    "\n",
    "speed_stream_pg19_512 = benchmark_speed_streaming_kv(model, tokenizer, prompt_pg19, n_sink=8, window_size=512, num_tokens=1000, batch_size=1)\n",
    "print(\"StreamingLLM(512) speed on PG-19 (500+1000 tokens):\", speed_stream_pg19_512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "少token生成实验（token总量不超出pythia2.8B模型的训练长度2048）\n",
      "\n",
      "=== WikiText 汇总 ===\n",
      "Baseline - PPL: 13.313, Speed: 20.85 tok/s\n",
      "Streaming(256) - PPL: 150.125, Speed: 11.67 tok/s\n",
      "Streaming(512) - PPL: 42.688, Speed: 16.77 tok/s\n",
      "\n",
      "=== PG-19 汇总 ===\n",
      "Baseline - PPL: 8.541, Speed: 18.10 tok/s\n",
      "Streaming(256) - PPL: 78.500, Speed: 11.50 tok/s\n",
      "Streaming(512) - PPL: 35.125, Speed: 16.83 tok/s\n"
     ]
    }
   ],
   "source": [
    "print(\"少token生成实验（token总量不超出pythia2.8B模型的训练长度2048）\")\n",
    "print(\"\\n=== WikiText 汇总 ===\")\n",
    "print(f\"Baseline - PPL: {ppl_base_wiki:.3f}, Speed: {speed_base_wiki['throughput']:.2f} tok/s\")\n",
    "print(f\"Streaming(256) - PPL: {ppl_stream_wiki_256:.3f}, Speed: {speed_stream_wiki_256['throughput']:.2f} tok/s\")\n",
    "print(f\"Streaming(512) - PPL: {ppl_stream_wiki_512:.3f}, Speed: {speed_stream_wiki_512['throughput']:.2f} tok/s\")\n",
    "print(\"\\n=== PG-19 汇总 ===\")\n",
    "print(f\"Baseline - PPL: {ppl_base_pg19:.3f}, Speed: {speed_base_pg19['throughput']:.2f} tok/s\")\n",
    "print(f\"Streaming(256) - PPL: {ppl_stream_pg19_256:.3f}, Speed: {speed_stream_pg19_256['throughput']:.2f} tok/s\")\n",
    "print(f\"Streaming(512) - PPL: {ppl_stream_pg19_512:.3f}, Speed: {speed_stream_pg19_512['throughput']:.2f} tok/s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPL测试\n",
    "text_wiki_ppl = text_wiki[:15000]  # 3000 tokens\n",
    "ppl_base_wiki_max = evaluate_ppl_baseline(model, tokenizer, text_wiki_ppl, max_tokens=3000)\n",
    "print(\"Baseline PPL on WikiText (3000 tokens):\", ppl_base_wiki_max)\n",
    "\n",
    "ppl_stream_wiki_256_max = evaluate_ppl_streaming_kv(model, tokenizer, text_wiki_ppl, n_sink=8, window_size=256, max_tokens=3000)\n",
    "print(\"StreamingLLM(256) PPL on WikiText (3000 tokens):\", ppl_stream_wiki_256_max)\n",
    "\n",
    "ppl_stream_wiki_512_max = evaluate_ppl_streaming_kv(model, tokenizer, text_wiki_ppl, n_sink=8, window_size=512, max_tokens=3000)\n",
    "print(\"StreamingLLM(512) PPL on WikiText (3000 tokens):\", ppl_stream_wiki_512_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加速测试\n",
    "prompt_wiki = text_wiki[:5000]\n",
    "\n",
    "speed_stream_wiki_256_max = benchmark_speed_streaming_kv(model, tokenizer, prompt_wiki, n_sink=8, window_size=256, num_tokens=2000, batch_size=1)\n",
    "print(\"StreamingLLM(256) speed on WikiText (1000+2000 tokens):\", speed_stream_wiki_256_max)\n",
    "\n",
    "speed_stream_wiki_512_max = benchmark_speed_streaming_kv(model, tokenizer, prompt_wiki, n_sink=8, window_size=512, num_tokens=2000, batch_size=1)\n",
    "print(\"StreamingLLM(512) speed on WikiText (1000+2000 tokens):\", speed_stream_wiki_512_max)\n",
    "\n",
    "speed_base_wiki_max = benchmark_speed_baseline(model, tokenizer, prompt_wiki, num_tokens=2000, batch_size=1)\n",
    "print(\"Baseline speed on WikiText (1000+2000 tokens):\", speed_base_wiki_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPL测试  \n",
    "text_pg19_ppl = text_pg19[:15000]  # 3000 tokens\n",
    "ppl_base_pg19_max = evaluate_ppl_baseline(model, tokenizer, text_pg19_ppl, max_tokens=3000)\n",
    "print(\"Baseline PPL on PG-19 (3000 tokens):\", ppl_base_pg19_max)\n",
    "\n",
    "ppl_stream_pg19_256_max = evaluate_ppl_streaming_kv(model, tokenizer, text_pg19_ppl, n_sink=8, window_size=256, max_tokens=3000)\n",
    "print(\"StreamingLLM(256) PPL on PG-19 (3000 tokens):\", ppl_stream_pg19_256_max)\n",
    "\n",
    "ppl_stream_pg19_512_max = evaluate_ppl_streaming_kv(model, tokenizer, text_pg19_ppl, n_sink=8, window_size=512, max_tokens=3000)\n",
    "print(\"StreamingLLM(512) PPL on PG-19 (3000 tokens):\", ppl_stream_pg19_512_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加速测试\n",
    "prompt_pg19 = text_pg19[:5000]\n",
    "speed_base_pg19_max = benchmark_speed_baseline(model, tokenizer, prompt_pg19, num_tokens=2000, batch_size=1)\n",
    "print(\"Baseline speed on PG-19 (1000+2000 tokens):\", speed_base_pg19_max)\n",
    "\n",
    "speed_stream_pg19_256_max = benchmark_speed_streaming_kv(model, tokenizer, prompt_pg19, n_sink=8, window_size=256, num_tokens=2000, batch_size=1)\n",
    "print(\"StreamingLLM(256) speed on PG-19 (1000+2000 tokens):\", speed_stream_pg19_256_max)\n",
    "\n",
    "speed_stream_pg19_512_max = benchmark_speed_streaming_kv(model, tokenizer, prompt_pg19, n_sink=8, window_size=512, num_tokens=2000, batch_size=1)\n",
    "print(\"StreamingLLM(512) speed on PG-19 (1000+2000 tokens):\", speed_stream_pg19_512_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"大量token生成实验（token总量超出pythia2.8B模型的训练长度2048）\")\n",
    "print(\"\\n=== WikiText 汇总 ===\")\n",
    "print(f\"Baseline - PPL: {ppl_base_wiki_max:.3f}, Speed: {speed_base_wiki_max['throughput']:.2f} tok/s\")\n",
    "print(f\"Streaming(256) - PPL: {ppl_stream_wiki_256_max:.3f}, Speed: {speed_stream_wiki_256_max['throughput']:.2f} tok/s\")\n",
    "print(f\"Streaming(512) - PPL: {ppl_stream_wiki_512_max:.3f}, Speed: {speed_stream_wiki_512_max['throughput']:.2f} tok/s\")\n",
    "print(\"\\n=== PG-19 汇总 ===\")\n",
    "print(f\"Baseline - PPL: {ppl_base_pg19_max:.3f}, Speed: {speed_base_pg19_max['throughput']:.2f} tok/s\")\n",
    "print(f\"Streaming(256) - PPL: {ppl_stream_pg19_256_max:.3f}, Speed: {speed_stream_pg19_256_max['throughput']:.2f} tok/s\")\n",
    "print(f\"Streaming(512) - PPL: {ppl_stream_pg19_512_max:.3f}, Speed: {speed_stream_pg19_512_max['throughput']:.2f} tok/s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
