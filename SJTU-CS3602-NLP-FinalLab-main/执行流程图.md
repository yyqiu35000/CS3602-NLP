# StreamingLLM 执行流程图

## 1. 初始化阶段

```
用户代码:
  press = PythiaStreamingLLMPress(max_capacity=256, n_sink=4)
  press.register(model)
    │
    ├─> 遍历 model.gpt_neox.layers (32层)
    │
    ├─> 对每个 layer.attention 注册 Hook:
    │     register_forward_pre_hook(hook_function, with_kwargs=True)
    │
    └─> 现在每个 Attention 层都被"监听"了！
```

## 2. 生成第一个 Token

```
model.generate(..., use_cache=True)
  │
  ├─> Token 0 进入模型
  │
  ├─> Layer 0 Attention.forward() 被调用
  │     │
  │     ├─> 🪝 我们的 Hook 被触发！
  │     │     _pre_forward_hook(module, args, kwargs, layer_idx=0)
  │     │       │
  │     │       ├─> cache = kwargs.get("layer_past")
  │     │       └─> cache is None (第一次，还没有 KV Cache)
  │     │       └─> return args, kwargs (不压缩)
  │     │
  │     └─> Attention 正常计算
  │           output, new_kv = attention(input)
  │           KV Cache 被创建: cache[0] = (key[1], value[1])
  │
  ├─> Layer 1-31 同样的过程...
  │
  └─> 生成 Token 1
      KV Cache 现在有 1 个 token 的历史
```

## 3. 生成中间 Token (Token 2-256)

```
生成 Token 2:
  │
  ├─> Token 1 + KV[0] 进入模型
  │
  ├─> Layer 0 Attention.forward()
  │     │
  │     ├─> 🪝 Hook 触发
  │     │     │
  │     │     ├─> cache = kwargs["layer_past"]  # 有了！
  │     │     ├─> key, value = cache[0]
  │     │     ├─> seq_len = key.shape[2]  # 现在是 2
  │     │     ├─> seq_len <= 256? Yes
  │     │     └─> return (不压缩，还没到上限)
  │     │
  │     └─> Attention 计算
  │           output, new_kv = attention(token_1, past_kv)
  │           更新 cache[0] = (key[2], value[2])
  │
  └─> 生成 Token 2

... 重复到 Token 256，KV Cache 逐步增长到 256
```

## 4. 触发压缩！(Token 257)

```
生成 Token 257:
  │
  ├─> Token 256 + KV[0:255] 进入模型
  │
  ├─> Layer 0 Attention.forward()
  │     │
  │     ├─> 🪝 Hook 触发
  │     │     │
  │     │     ├─> cache = kwargs["layer_past"]
  │     │     ├─> key, value = cache[0]
  │     │     ├─> seq_len = 257  # 超过上限了！
  │     │     │
  │     │     ├─> ⚠️ 开始压缩！
  │     │     │     │
  │     │     │     ├─> k_sink = key[:, :, 0:4, :]        # 保留前4个
  │     │     │     ├─> k_window = key[:, :, -252:, :]     # 保留最后252个
  │     │     │     ├─> k_new = cat([k_sink, k_window])   # 拼接
  │     │     │     │
  │     │     │     ├─> 丢弃: key[:, :, 4:5, :]  (第5个token)
  │     │     │     │
  │     │     │     └─> cache.layers[0].keys = k_new      # 修改 Cache！
  │     │     │           cache.layers[0].values = v_new
  │     │     │           seq_len: 257 -> 256 ✅
  │     │     │
  │     │     └─> return modified_kwargs
  │     │
  │     └─> Attention 用压缩后的 KV 计算
  │           output, new_kv = attention(token_256, compressed_kv[256])
  │           cache[0] = (key[256], value[256])  # 仍然是 256
  │
  ├─> Layer 1-31 同样压缩...
  │
  └─> 生成 Token 257
      compression_count += 32 (每层压缩一次)
```

## 5. 后续 Token (258-2000)

```
生成 Token 258:
  │
  ├─> 每次都会触发压缩
  ├─> 每次都丢弃第 5 个 token
  └─> KV Cache 始终保持 256 大小

KV Cache 内容演化:
  Token 256: [0,1,2,3, 4, 5, 6, ..., 255]
  Token 257: [0,1,2,3, 5, 6, 7, ..., 256]  ← 丢弃了 token 4
  Token 258: [0,1,2,3, 6, 7, 8, ..., 257]  ← 丢弃了 token 5
  Token 259: [0,1,2,3, 7, 8, 9, ..., 258]  ← 丢弃了 token 6
  ...
  Token 2000: [0,1,2,3, 1748, ..., 1999]   ← 丢弃了 token 4-1747
  
  始终保留:
    - 前 4 个 (Attention Sinks)
    - 最后 252 个 (滑动窗口)
```

## 6. 内存对比

```
Baseline (无压缩):
  ───────────────────────────────────────────────
  Token 2000 时的 KV Cache:
    Shape: [batch=1, heads=32, seq=2000, dim=128]
    Memory: 2000 * 32 * 128 * 2 (bytes) * 32 (layers)
          ≈ 524 MB 💥
  ───────────────────────────────────────────────

StreamingLLM (压缩):
  ───────────────────────────────────────────────
  Token 2000 时的 KV Cache:
    Shape: [batch=1, heads=32, seq=256, dim=128]
    Memory: 256 * 32 * 128 * 2 (bytes) * 32 (layers)
          ≈ 67 MB ✅
  ───────────────────────────────────────────────
  
  节省: 524 - 67 = 457 MB (87% 内存节省！)
```

## 7. Hook 机制可视化

```
PyTorch 执行流程:

  用户代码: outputs = model.generate(...)
                        │
                        ↓
  ┌─────────────────────────────────────────┐
  │  Transformers 内部:                      │
  │                                         │
  │  for layer in model.layers:            │
  │      │                                  │
  │      ├─> 准备调用 layer.attention()    │
  │      │                                  │
  │      ├─> 🪝 触发我们的 Hook!             │
  │      │     _pre_forward_hook()         │
  │      │       │                          │
  │      │       ├─> 拦截 kwargs            │
  │      │       ├─> 压缩 KV Cache         │
  │      │       └─> 返回修改后的 kwargs   │
  │      │                                  │
  │      └─> 调用真正的 attention.forward() │
  │            (用压缩后的 KV)              │
  │                                         │
  └─────────────────────────────────────────┘
                        │
                        ↓
                   生成下一个 token
```

## 8. 为什么这个设计很优雅？

```
传统方案 (修改源码):
  ❌ Transformers 源码
       ↓ 修改
  ❌ Attention.forward()
       ↓ 添加压缩逻辑
  ❌ 难维护、升级困难

我们的方案 (Hook):
  ✅ Transformers 源码 (完全不动)
       ↓
  ✅ Attention.forward() (完全不动)
       ↑
  ✅ 我们的 Hook (外部注入)
       ├─> 拦截参数
       ├─> 压缩 KV
       └─> 透明返回

  优点:
    - 代码隔离
    - 易于开关
    - 支持任意模型
    - 调试方便
```

## 9. 关键代码位置

```
pythia_press.py:
  
  第 16-45 行:   __init__() - 初始化参数
  第 47-53 行:   _make_hook() - 创建 Hook 函数
  第 55-132 行:  _pre_forward_hook() - 核心压缩逻辑 ⭐
                   ├─> 第 59 行: 拦截 cache
                   ├─> 第 94 行: 切片保留 Sinks
                   ├─> 第 96 行: 切片保留 Window
                   ├─> 第 98 行: 拼接
                   └─> 第 122 行: 修改 cache
  第 134-166 行: register() - 注册 Hook
  第 168-171 行: remove() - 移除 Hook
```

## 10. 完整执行时间线

```
时间轴:
  
  T=0:    press.register(model)
          └─> 32 个 Attention 层被监听
  
  T=1:    model.generate() 开始
  
  T=2-256: 生成前 256 个 token
          └─> Hook 触发 8192 次 (32层 × 256步)
          └─> 但不压缩 (seq_len <= 256)
  
  T=257:  第一次压缩！
          └─> Hook 触发 32 次并压缩
          └─> KV: 257 -> 256
  
  T=258-2000: 每步都压缩
          └─> Hook 触发 55,776 次 (32层 × 1744步)
          └─> 总压缩次数: 55,776
  
  T=2001: 生成完成
          └─> press.remove() 移除 Hook
```

这就是整个 StreamingLLM 的执行流程！每一步都清清楚楚！🎯
