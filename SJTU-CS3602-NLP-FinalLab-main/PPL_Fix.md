# PPL è®¡ç®—ä¿®å¤è¯´æ˜

## ğŸ¯ é—®é¢˜è¯Šæ–­

### åŸå§‹é—®é¢˜
åœ¨ 70%-80% é«˜å‹ç¼©ç‡ä¸‹ï¼ŒStreamingLLM çš„ PPL åº”è¯¥ä¸Šå‡ï¼ˆå› ä¸ºä¸¢å¼ƒäº†å¤§é‡ KV Cacheï¼‰ï¼Œä½†å®é™…æµ‹è¯•æ˜¾ç¤ºï¼š
```
PPL (Baseline):     9.79
PPL (StreamingLLM): 9.79  â† å®Œå…¨ç›¸åŒï¼Œä¸åˆç†ï¼
```

### æ ¹æœ¬åŸå› 

**æ—§çš„ `calculate_ppl` å®ç°ï¼š**
```python
def calculate_ppl(text, stride=512):
    # ...
    with torch.no_grad():
        outputs = model(input_ids, labels=target_ids)  # âŒ æ²¡æœ‰ use_cache=True
        neg_log_likelihood = outputs.loss * trg_len
```

**é—®é¢˜åˆ†æï¼š**
1. âŒ ç›´æ¥è°ƒç”¨ `model(input_ids, labels=...)`
2. âŒ æ²¡æœ‰ä¼ å…¥ `use_cache=True`ï¼Œä¸ç”Ÿæˆ `past_key_values`
3. âŒ æ²¡æœ‰ KV Cache â†’ StreamingLLM çš„ pre-forward hook **æ ¹æœ¬ä¸è§¦å‘**
4. âŒ Baseline å’Œ StreamingLLM çš„ PPL è®¡ç®—**å®Œå…¨ç›¸åŒ**ï¼ˆéƒ½ä¸ä½¿ç”¨ç¼“å­˜ï¼‰

**å¯¹æ¯”ï¼šä¸ºä»€ä¹ˆ speed æµ‹è¯•æœ‰æ•ˆï¼Ÿ**
```python
model.generate(..., use_cache=True)  # âœ… generate ä½¿ç”¨ KV Cacheï¼Œhook ç”Ÿæ•ˆ
```

---

## âœ… è§£å†³æ–¹æ¡ˆ

### ä¿®æ”¹å†…å®¹

æ–°å¢ `use_kv_cache` å‚æ•°ï¼Œæ”¯æŒä¸¤ç§æ¨¡å¼ï¼š

#### æ¨¡å¼ 1ï¼šå¿«é€Ÿæ¨¡å¼ï¼ˆuse_kv_cache=Falseï¼Œé»˜è®¤ï¼‰
- ä¸ä½¿ç”¨ KV Cache
- è®¡ç®—é€Ÿåº¦å¿«
- **ä¸åæ˜ ** StreamingLLM å‹ç¼©çš„å½±å“
- é€‚åˆï¼šå¿«é€ŸåŸºå‡†æµ‹è¯•

#### æ¨¡å¼ 2ï¼šçœŸå®æ¨¡å¼ï¼ˆuse_kv_cache=Trueï¼‰
- é€ token è®¡ç®—ï¼Œç´¯ç§¯ KV Cache
- StreamingLLM hook **ä¼šçœŸå®è§¦å‘**
- **çœŸå®åæ˜ **å‹ç¼©å¯¹è´¨é‡çš„å½±å“
- é€‚åˆï¼šéªŒè¯ StreamingLLM æ•ˆæœ

### æ–°çš„å®ç°

```python
def calculate_ppl(text, stride=512, use_kv_cache=False):
    if not use_kv_cache:
        # åŸå§‹å¿«é€Ÿæ–¹æ³•ï¼ˆä¸ä½¿ç”¨ KV Cacheï¼‰
        # ...
    else:
        # æ–°æ–¹æ³•ï¼šé€ token è®¡ç®—ï¼Œä½¿ç”¨ KV Cache
        input_ids = encodings.input_ids[:, :max_test_len].to(DEVICE)
        past_key_values = None
        
        for i in range(1, input_ids.size(1)):
            if i == 1:
                current_input = input_ids[:, :i]
            else:
                current_input = input_ids[:, i:i+1]
            
            with torch.no_grad():
                outputs = model(
                    current_input,
                    past_key_values=past_key_values,
                    use_cache=True,  # âœ… å…³é”®ï¼šå¯ç”¨ KV Cache
                    return_dict=True
                )
                
                # è®¡ç®—å½“å‰ token çš„ loss
                logits = outputs.logits[:, -1, :]
                target = input_ids[:, i]
                loss = torch.nn.functional.cross_entropy(
                    logits.unsqueeze(0), 
                    target.unsqueeze(0)
                )
                nlls.append(loss)
                
                # âœ… æ›´æ–° past_key_valuesï¼ˆä¼šè¢« StreamingLLM å‹ç¼©ï¼ï¼‰
                past_key_values = outputs.past_key_values
```

### è°ƒç”¨æ–¹å¼

```python
# Baseline å’Œ StreamingLLM éƒ½ä½¿ç”¨çœŸå®æ¨¡å¼
results["Baseline"] = run_benchmark_suite("Baseline", use_kv_cache_for_ppl=True)

with press(model):
    results["StreamingLLM"] = run_benchmark_suite("StreamingLLM", use_kv_cache_for_ppl=True)
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

ä¿®å¤åï¼ŒPPL åº”è¯¥ä¼šå‡ºç°åˆç†çš„å˜åŒ–ï¼š

### å‹ç¼©ç‡ 0.7 (ä¿ç•™ 30%)
```
PPL (Baseline):     9.79
PPL (StreamingLLM): 10.2 - 11.5  â† é¢„æœŸä¸Šå‡ 4-17%
```

### ä¸ºä»€ä¹ˆ PPL ä¼šä¸Šå‡ï¼Ÿ

1. **ä¿¡æ¯ä¸¢å¤±**ï¼šä¸¢å¼ƒ 70% çš„ä¸­é—´ tokens çš„ KV Cache
2. **ä¸Šä¸‹æ–‡å‡å°‘**ï¼šæ¨¡å‹åªèƒ½çœ‹åˆ° Sink tokens + æœ€è¿‘çš„ tokens
3. **é•¿è·ç¦»ä¾èµ–**ï¼šé•¿è·ç¦»ä¾èµ–è¢«åˆ‡æ–­ï¼Œå½±å“é¢„æµ‹å‡†ç¡®åº¦

### åˆç†çš„ PPL ä¸Šå‡èŒƒå›´

| å‹ç¼©ç‡ | ä¿ç•™æ¯”ä¾‹ | é¢„æœŸ PPL ä¸Šå‡ | è¯´æ˜         |
| ------ | -------- | ------------- | ------------ |
| 0.5    | 50%      | +2-5%         | ä¿å®ˆå‹ç¼©     |
| 0.7    | 30%      | **+4-17%**    | **å½“å‰é…ç½®** |
| 0.8    | 20%      | +10-25%       | æ¿€è¿›å‹ç¼©     |

å‚è€ƒï¼šStreamingLLM è®ºæ–‡æ˜¾ç¤ºåœ¨ç±»ä¼¼å‹ç¼©ç‡ä¸‹ï¼ŒPPL ä¸Šå‡çº¦ 5-15%ï¼Œè¿™æ˜¯**å¯æ¥å—çš„è´¨é‡ä»£ä»·**ã€‚

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. è®¡ç®—æ—¶é—´å¢åŠ 
```
å¿«é€Ÿæ¨¡å¼ï¼š~30 ç§’
çœŸå®æ¨¡å¼ï¼š~3-5 åˆ†é’Ÿ â¬†ï¸
```

**åŸå› **ï¼šé€ token è®¡ç®— + KV Cache ç®¡ç†

### 2. æ˜¾å­˜å ç”¨
çœŸå®æ¨¡å¼ä¼šå ç”¨æ›´å¤šæ˜¾å­˜ï¼ˆéœ€è¦å­˜å‚¨ past_key_valuesï¼‰

### 3. å¦‚ä½•é€‰æ‹©æ¨¡å¼ï¼Ÿ

- **è°ƒè¯•é˜¶æ®µ**ï¼šç”¨å¿«é€Ÿæ¨¡å¼ï¼ˆ`use_kv_cache=False`ï¼‰
- **æœ€ç»ˆéªŒè¯**ï¼šç”¨çœŸå®æ¨¡å¼ï¼ˆ`use_kv_cache=True`ï¼‰
- **è®ºæ–‡/æŠ¥å‘Š**ï¼šå¿…é¡»ç”¨çœŸå®æ¨¡å¼ï¼Œæ‰èƒ½åæ˜ çœŸå®æ•ˆæœ

---

## ğŸ” éªŒè¯æ–¹æ³•

è¿è¡Œä¿®å¤åçš„è„šæœ¬ï¼š
```bash
python benchmark_streaming.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
[Baseline] WikiText PPL: 9.79
[StreamingLLM] WikiText PPL: 10.5  â† åº”è¯¥ä¸Šå‡ï¼

æŒ‡æ ‡                   | Baseline | StreamingLLM | å˜åŒ–
---------------------------------------------------------
PPL (Lower is better)  | 9.79     | 10.50        | +7.2% âœ…
Memory (MB)            | 6100.46  | 5493.35      | -10.0% âœ…
Throughput (t/s)       | 24.40    | 32.68        | +33.9% âœ…
```

**åˆ¤æ–­æ ‡å‡†**ï¼š
- âœ… PPL ä¸Šå‡ < 20%ï¼šæ•ˆæœè‰¯å¥½
- âš ï¸ PPL ä¸Šå‡ 20-30%ï¼šå¯æ¥å—
- âŒ PPL ä¸Šå‡ > 30%ï¼šå‹ç¼©ç‡è¿‡é«˜

---

## ğŸ“ æ€»ç»“

### ä¿®å¤å‰
```python
# âŒ PPL è®¡ç®—ä¸ä½¿ç”¨ KV Cache
calculate_ppl(text)  # ä¸è§¦å‘ StreamingLLM
â†’ PPL æ°¸è¿œç›¸åŒ
```

### ä¿®å¤å
```python
# âœ… PPL è®¡ç®—ä½¿ç”¨ KV Cache
calculate_ppl(text, use_kv_cache=True)  # è§¦å‘ StreamingLLM
â†’ PPL çœŸå®åæ˜ å‹ç¼©å½±å“
```

### å…³é”®æ”¹è¿›
1. âœ… æ–°å¢ `use_kv_cache` å‚æ•°
2. âœ… é€ token è®¡ç®—ï¼Œç´¯ç§¯ past_key_values
3. âœ… StreamingLLM hook æ­£ç¡®è§¦å‘
4. âœ… PPL çœŸå®åæ˜ è´¨é‡å˜åŒ–

---

**ä¿®å¤æ—¶é—´**: 2024-12-14  
**å½±å“æ–‡ä»¶**: `benchmark_streaming.py`  
**æµ‹è¯•çŠ¶æ€**: å¾…éªŒè¯
