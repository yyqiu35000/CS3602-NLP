# é¡¹ç›®æ ¸å¿ƒå®ç°åŸç†è¯¦è§£

## é—®é¢˜ï¼šä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”¨ç°æˆçš„åº“ï¼Ÿ

### ä½ å¯èƒ½æƒ³çš„æ–¹æ¡ˆï¼ˆä½†è¡Œä¸é€šï¼‰ï¼š

```python
# âŒ é”™è¯¯æƒ³æ³•1ï¼šç›´æ¥ç”¨ transformers çš„ generate
model.generate(..., use_cache=True)  
# é—®é¢˜ï¼šuse_cache=True åªæ˜¯å¯ç”¨KV Cacheï¼Œä½†ä¸ä¼šè‡ªåŠ¨å‹ç¼©ï¼

# âŒ é”™è¯¯æƒ³æ³•2ï¼šç”¨ kvpress åº“
from kvpress import KnowledgePreservingPress
# é—®é¢˜ï¼škvpress ä¸æ”¯æŒ Pythia/GPT-NeoX æ¶æ„ï¼Œè€Œä¸”æˆ‘ä»¬æƒ³è‡ªå·±å®ç°å­¦ä¹ 

# âŒ é”™è¯¯æƒ³æ³•3ï¼šä¿®æ”¹æ¨¡å‹çš„ forward å‡½æ•°
# é—®é¢˜ï¼šå¤ªå¤æ‚ï¼Œéœ€è¦æ”¹åŠ¨ transformers æºç 
```

**æ ¸å¿ƒé—®é¢˜**ï¼šTransformers åº“çš„ `generate()` å’Œ `forward()` è™½ç„¶æ”¯æŒ KV Cacheï¼Œä½†å®ƒä»¬ï¼š
1. **ä¸ä¼šè‡ªåŠ¨å‹ç¼©** KV Cache
2. **æ²¡æœ‰æä¾›æ¥å£**è®©ä½ åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿®æ”¹ KV Cache
3. KV Cache æ˜¯**å†…éƒ¨ç®¡ç†**çš„ï¼Œå¤–éƒ¨æ— æ³•ç›´æ¥è®¿é—®

## è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ PyTorch Hook æœºåˆ¶

### ä»€ä¹ˆæ˜¯ Hookï¼Ÿ

Hookï¼ˆé’©å­ï¼‰æ˜¯ PyTorch æä¾›çš„ä¸€ç§æœºåˆ¶ï¼Œå¯ä»¥è®©ä½ **æ‹¦æˆª**æ¨¡å‹çš„å‰å‘/åå‘ä¼ æ’­è¿‡ç¨‹ï¼Œå¹¶åœ¨**ä¸ä¿®æ”¹æºç **çš„æƒ…å†µä¸‹æ’å…¥è‡ªå®šä¹‰é€»è¾‘ã€‚

```python
# Hook çš„æœ¬è´¨
class Module:
    def forward(self, x):
        # 1. è°ƒç”¨ pre_forward_hook (æˆ‘ä»¬åœ¨è¿™é‡Œæ‹¦æˆª!)
        x = self.pre_forward_hook(x)
        
        # 2. æ‰§è¡Œæ­£å¸¸çš„ forward
        output = self.do_computation(x)
        
        # 3. è°ƒç”¨ post_forward_hook
        return self.post_forward_hook(output)
```

### æˆ‘ä»¬çš„å®ç°æ­¥éª¤

#### ç¬¬1æ­¥ï¼šæ‰¾åˆ°ç›®æ ‡å±‚ï¼ˆAttentionå±‚ï¼‰

```python
# pythia_press.py ç¬¬ 135-155 è¡Œ
def register(self, model):
    # æ‰¾åˆ° Pythia æ¨¡å‹çš„æ‰€æœ‰ Attention å±‚
    layers = model.gpt_neox.layers  # 32 ä¸ª transformer å±‚
    
    for i, layer in enumerate(layers):
        target = layer.attention  # æ¯å±‚çš„ Attention æ¨¡å—
        
        # æ³¨å†Œ Hook åˆ°è¿™ä¸ªå±‚
        handle = target.register_forward_pre_hook(
            self._make_hook(i), 
            with_kwargs=True  # é‡è¦ï¼è®©æˆ‘ä»¬è®¿é—® kwargs
        )
```

**ä¸ºä»€ä¹ˆæ˜¯ Attention å±‚ï¼Ÿ**
- KV Cache åªå­˜åœ¨äº Attention å±‚
- æ¯ä¸ª token çš„ Key å’Œ Value éƒ½å­˜å‚¨åœ¨è¿™é‡Œ
- è¿™æ˜¯ StreamingLLM å‹ç¼©çš„ç›®æ ‡

#### ç¬¬2æ­¥ï¼šæ‹¦æˆª KV Cacheï¼ˆDynamicCacheï¼‰

```python
# pythia_press.py ç¬¬ 56-77 è¡Œ
def _pre_forward_hook(self, module, args, kwargs, layer_idx):
    # åœ¨ forward æ‰§è¡Œä¹‹å‰è¢«è°ƒç”¨ï¼
    
    # 1. ä» kwargs ä¸­è·å– KV Cache
    cache = kwargs.get("layer_past")  # DynamicCache å¯¹è±¡
    
    if cache is None:
        return args, kwargs  # ç¬¬ä¸€æ¬¡ï¼Œè¿˜æ²¡æœ‰ cache
    
    # 2. è®¿é—®å½“å‰å±‚çš„ KV Cache
    kv_tuple = cache[layer_idx]  # è¿”å› (key_tensor, value_tensor)
    key, value = kv_tuple
    
    # 3. æ£€æŸ¥ KV Cache çš„å¤§å°
    seq_len = key.shape[2]  # [batch, num_heads, seq_len, head_dim]
    print(f"å½“å‰åºåˆ—é•¿åº¦: {seq_len}")
```

**å…³é”®å‘ç°**ï¼š
- Transformers ä¼šæŠŠ KV Cache ä»¥ `layer_past` å‚æ•°ä¼ ç»™æ¯ä¸ª Attention å±‚
- æˆ‘ä»¬å¯ä»¥é€šè¿‡ `with_kwargs=True` è®¿é—®è¿™ä¸ªå‚æ•°
- DynamicCache æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰å±‚çš„ KV tensors

#### ç¬¬3æ­¥ï¼šæ‰‹åŠ¨å‹ç¼© KV Cache

```python
# pythia_press.py ç¬¬ 84-98 è¡Œ
# åˆ¤æ–­æ˜¯å¦éœ€è¦å‹ç¼©
if seq_len <= self.max_capacity:
    return args, kwargs  # è¿˜ä¸éœ€è¦å‹ç¼©

# å¼€å§‹å‹ç¼©ï¼
window_size = self.max_capacity - self.n_sink

# ä¿ç•™å‰ n_sink ä¸ª tokens (Attention Sinks)
k_sink = key[:, :, :self.n_sink, :]      # å‰ 4 ä¸ª
v_sink = value[:, :, :self.n_sink, :]

# ä¿ç•™æœ€å window_size ä¸ª tokens (æ»‘åŠ¨çª—å£)
k_window = key[:, :, -window_size:, :]   # æœ€å 252 ä¸ª
v_window = value[:, :, -window_size:, :]

# æ‹¼æ¥ï¼š[Sinks] + [Window]
k_new = torch.cat([k_sink, k_window], dim=2)
v_new = torch.cat([v_sink, v_window], dim=2)

# ç»“æœï¼šä» seq_len å‹ç¼©åˆ° max_capacity (256)
```

**è¿™å°±æ˜¯ StreamingLLM çš„æ ¸å¿ƒç®—æ³•ï¼**

```
åŸå§‹ KV Cache (å‡è®¾ 512 tokens):
[0] [1] [2] [3] [4] ... [256] [257] ... [510] [511]

å‹ç¼©å (256 tokens):
[0] [1] [2] [3] â†Sinks  [260] [261] ... [510] [511] â†Window
                â†‘
            ä¸¢å¼ƒä¸­é—´çš„ï¼
```

#### ç¬¬4æ­¥ï¼šä¿®æ”¹ DynamicCache å¯¹è±¡

```python
# pythia_press.py ç¬¬ 118-122 è¡Œ
# æŠŠå‹ç¼©åçš„ KV å†™å› cache
if hasattr(cache, "layers") and layer_idx < len(cache.layers):
    cache.layers[layer_idx].keys = k_new    # æ›¿æ¢ Key
    cache.layers[layer_idx].values = v_new  # æ›¿æ¢ Value

return args, kwargs  # è¿”å›ä¿®æ”¹åçš„å‚æ•°
```

**å…³é”®**ï¼š
- æˆ‘ä»¬ç›´æ¥ä¿®æ”¹äº† DynamicCache å¯¹è±¡çš„å†…éƒ¨å­˜å‚¨
- åç»­çš„è®¡ç®—ä¼šä½¿ç”¨å‹ç¼©åçš„ KV Cache
- Attention è®¡ç®—ä» O(512Â²) é™åˆ° O(256Â²)

## å®Œæ•´çš„è°ƒç”¨æµç¨‹

```python
# benchmark_streaming.py ä½¿ç”¨ç¤ºä¾‹

# 1. åˆ›å»ºå‹ç¼©å™¨
press = PythiaStreamingLLMPress(
    compression_ratio=0.7,
    n_sink=4,
    max_capacity=256
)

# 2. æ³¨å†Œ Hookï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
press.register(model)
# æ­¤æ—¶ï¼Œ32 ä¸ª Attention å±‚éƒ½è¢«"ç›‘å¬"äº†

# 3. æ­£å¸¸ä½¿ç”¨æ¨¡å‹
with torch.no_grad():
    outputs = model.generate(
        input_ids,
        max_new_tokens=2000,
        use_cache=True  # å¿…é¡»å¼€å¯ï¼
    )

# æ¯æ¬¡ç”Ÿæˆ token æ—¶ï¼š
# Step 1: æ–° token è¿›å…¥æ¨¡å‹
# Step 2: æ¯ä¸ª Attention å±‚çš„ forward è¢«è°ƒç”¨
# Step 3: ğŸª æˆ‘ä»¬çš„ Hook è¢«è§¦å‘ï¼
# Step 4: æ£€æŸ¥ KV Cache å¤§å° -> å¦‚æœè¶…è¿‡ 256ï¼Œå‹ç¼©ï¼
# Step 5: ç”¨å‹ç¼©åçš„ KV ç»§ç»­è®¡ç®—
# Step 6: ç”Ÿæˆä¸‹ä¸€ä¸ª token
# ... é‡å¤

# 4. ç”Ÿæˆå®Œæˆåç§»é™¤ Hook
press.remove()
```

## ä¸ºä»€ä¹ˆè¿™ä¸ªæ–¹æ³•å¾ˆå·§å¦™ï¼Ÿ

### ä¼˜ç‚¹

1. âœ… **æ— éœ€ä¿®æ”¹æ¨¡å‹æºç **ï¼šé€šè¿‡ Hook æœºåˆ¶ï¼Œä¸åŠ¨ transformers åº“
2. âœ… **é€æ˜å‹ç¼©**ï¼šæ¨¡å‹ä¸çŸ¥é“ KV è¢«å‹ç¼©äº†ï¼Œç…§å¸¸è¿è¡Œ
3. âœ… **çµæ´»æ§åˆ¶**ï¼šå¯ä»¥éšæ—¶å¯ç”¨/ç¦ç”¨ï¼Œè°ƒæ•´å‹ç¼©å‚æ•°
4. âœ… **æ”¯æŒæ‰€æœ‰æ¨¡å¼**ï¼šgenerate(), forward() éƒ½èƒ½ç”¨

### æŒ‘æˆ˜

1. âš ï¸ **éœ€è¦ç†è§£å†…éƒ¨ç»“æ„**ï¼šå¿…é¡»çŸ¥é“ DynamicCache çš„å®ç°
2. âš ï¸ **æ¶æ„ä¾èµ–**ï¼šä¸åŒæ¨¡å‹çš„ Attention å±‚ç»“æ„ä¸åŒ
3. âš ï¸ **è°ƒè¯•å›°éš¾**ï¼šHook é‡Œçš„é”™è¯¯ä¸å®¹æ˜“å®šä½

## å…³é”®æŠ€æœ¯ç‚¹æ€»ç»“

| æŠ€æœ¯ç‚¹                                 | ä½œç”¨                      |
| -------------------------------------- | ------------------------- |
| `register_forward_pre_hook`            | åœ¨ forward ä¹‹å‰æ‹¦æˆª       |
| `with_kwargs=True`                     | è®¿é—® `layer_past` å‚æ•°    |
| `cache[layer_idx]`                     | è·å–å½“å‰å±‚çš„ (key, value) |
| `cache.layers[layer_idx].keys = k_new` | ä¿®æ”¹ KV Cache             |
| `torch.cat([sink, window], dim=2)`     | æ‹¼æ¥å‹ç¼©åçš„ tensors      |

## ä¸å…¶ä»–æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ                    | ä¼˜ç‚¹             | ç¼ºç‚¹                   |
| ----------------------- | ---------------- | ---------------------- |
| **æˆ‘ä»¬çš„ Hook æ–¹æ¡ˆ**    | çµæ´»ã€å¯æ§ã€é€šç”¨ | éœ€è¦ç†è§£å†…éƒ¨å®ç°       |
| ä¿®æ”¹ transformers æºç   | æœ€åº•å±‚æ§åˆ¶       | éš¾ç»´æŠ¤ã€å‡çº§å›°éš¾       |
| ä½¿ç”¨ kvpress åº“         | å¼€ç®±å³ç”¨         | ä¸æ”¯æŒæ‰€æœ‰æ¨¡å‹ã€ä¸é€æ˜ |
| è‡ªå·±å®ç° generate       | å®Œå…¨æ§åˆ¶         | å·¥ä½œé‡å·¨å¤§             |
| ç›´æ¥ç”¨ transformers API | æœ€ç®€å•           | âŒ æ— æ³•å‹ç¼© KV Cache    |

## éªŒè¯æˆ‘ä»¬çš„å®ç°æ˜¯å¦æ­£ç¡®

```python
# è¿è¡Œæ—¶ä¼šçœ‹åˆ°ï¼š
[StreamingLLM] æ³¨å†Œ Pre-Hook åˆ° 32 ä¸ª Attention å±‚...
[StreamingLLM] å‹ç¼©ç‡: 0.7, Sink: 4, MaxCapacity: 256

[COMPRESS DEBUG] Layer 0
  Seq Len: 300 -> Expected: 256, Actual: 256
  n_sink=4, window_size=252
  Attention Sinks Preserved: True  âœ…

# å‹ç¼©æ¬¡æ•°ç»Ÿè®¡
print(f"æ€»å‹ç¼©æ¬¡æ•°: {press.compression_count}")
# è¾“å‡ºï¼šçº¦ 768 æ¬¡ï¼ˆ32å±‚ Ã— æ¯å±‚å‹ç¼©è‹¥å¹²æ¬¡ï¼‰
```

## è¿™å°±æ˜¯æ•´ä¸ªé¡¹ç›®çš„æ ¸å¿ƒï¼

**ä¸€å¥è¯æ€»ç»“**ï¼š  
æˆ‘ä»¬é€šè¿‡ **PyTorch Hook** æœºåˆ¶ï¼Œåœ¨ Attention å±‚çš„ forward ä¹‹å‰**æ‹¦æˆªå¹¶ä¿®æ”¹** KV Cacheï¼Œå®ç°äº†åœ¨ä¸æ”¹åŠ¨æ¨¡å‹ä»£ç çš„å‰æä¸‹å®Œæˆ StreamingLLM å‹ç¼©ã€‚
