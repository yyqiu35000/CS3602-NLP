"""
StreamingLLM Press è°ƒè¯•è„šæœ¬
é€æ­¥éªŒè¯æ¯ä¸ªç¯èŠ‚æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

MODEL_PATH = "./models/pythia-70m"
DEVICE = "cuda"

print("=" * 60)
print("StreamingLLM Press è°ƒè¯•å·¥å…·")
print("=" * 60)

# åŠ è½½æ¨¡å‹
print("\n[1/6] åŠ è½½æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH, torch_dtype=torch.float16, device_map="auto"
)
model.eval()
print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")

# å‡†å¤‡è¾“å…¥
print("\n[2/6] å‡†å¤‡è¾“å…¥...")
prompt = "Hello, this is a test."
inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)
print(f"âœ“ Prompt tokens: {inputs.input_ids.shape[1]}")


# ========== è°ƒè¯•ç‰ˆæœ¬çš„ Press ==========
class DebugPress:
    def __init__(self, max_capacity=3, n_sink=1):
        self.max_capacity = max_capacity
        self.n_sink = n_sink
        self.hooks = []
        self.hook_call_count = 0
        self.compression_count = 0
        self.module_to_layer_idx = {}  # æ¨¡å—åˆ°å±‚ç´¢å¼•çš„æ˜ å°„

    def _make_hook(self, layer_idx):
        """ä¸ºæ¯ä¸ªå±‚åˆ›å»ºä¸“å±çš„ hook"""

        def hook(module, args, kwargs):
            return self._pre_forward_hook(module, args, kwargs, layer_idx)

        return hook

    def _pre_forward_hook(self, module, args, kwargs, layer_idx):
        """Pre-forward hook with kwargs: æ‹¦æˆªå¹¶å‹ç¼© DynamicCache ä¸­çš„ KV"""
        self.hook_call_count += 1
        verbose = self.hook_call_count <= 3  # åªæ‰“å°å‰3æ¬¡

        # æŸ¥æ‰¾ layer_past (DynamicCache å¯¹è±¡)
        cache = kwargs.get("layer_past")

        if cache is None:
            return args, kwargs

        # æ£€æŸ¥æ˜¯å¦æ˜¯ DynamicCache
        cache_type = type(cache).__name__
        if cache_type != "DynamicCache":
            return args, kwargs

        # ç®€åŒ–çš„è°ƒè¯•è¾“å‡º
        if verbose and layer_idx == 0:  # åªæ‰“å°ç¬¬0å±‚
            try:
                seq_len = cache.get_seq_length()
                print(
                    f"\n  [Hook #{self.hook_call_count}] Layer 0, cache seq_len={seq_len}"
                )
            except:
                pass

        # ä½¿ç”¨ cache[layer_idx] è®¿é—® KV tuple
        try:
            kv_tuple = cache[layer_idx]
        except:
            return args, kwargs

        if not isinstance(kv_tuple, tuple) or len(kv_tuple) != 2:
            return args, kwargs

        key, value = kv_tuple

        if key is None or value is None:
            return args, kwargs

        # seq_len åœ¨ç»´åº¦ 2: [batch, num_heads, seq_len, head_dim]
        seq_len = key.shape[2]

        # åˆ¤æ–­æ˜¯å¦éœ€è¦å‹ç¼©
        if seq_len <= self.max_capacity:
            return args, kwargs

        # æ‰§è¡Œå‹ç¼©
        self.compression_count += 1
        window_size = self.max_capacity - self.n_sink

        k_sink = key[:, :, : self.n_sink, :]
        v_sink = value[:, :, : self.n_sink, :]
        k_window = key[:, :, -window_size:, :]
        v_window = value[:, :, -window_size:, :]

        k_new = torch.cat([k_sink, k_window], dim=2)
        v_new = torch.cat([v_sink, v_window], dim=2)

        # åªåœ¨å‰10æ¬¡æˆ–æ¯50æ¬¡æ‰“å°
        if self.compression_count <= 10 or self.compression_count % 50 == 0:
            print(
                f"  [å‹ç¼© #{self.compression_count}] layer={layer_idx}, {seq_len} â†’ {k_new.shape[2]}"
            )

        # ä½¿ç”¨æ­£ç¡®çš„æ–¹å¼ä¿®æ”¹ DynamicCacheï¼šcache.layers[idx].keys/values
        try:
            # DynamicCache.__getitem__ è¿”å› self.layers[idx].keys, self.layers[idx].values
            # æ‰€ä»¥æˆ‘ä»¬åº”è¯¥ä¿®æ”¹ cache.layers[idx] çš„å±æ€§
            if hasattr(cache, "layers") and layer_idx < len(cache.layers):
                cache.layers[layer_idx].keys = k_new
                cache.layers[layer_idx].values = v_new

            # éªŒè¯å‹ç¼©æ˜¯å¦ç”Ÿæ•ˆ
            if self.compression_count <= 10:
                verify_kv = cache[layer_idx]
                if verify_kv[0] is not None:
                    actual_len = verify_kv[0].shape[2]
                    print(
                        f"    âœ… éªŒè¯: cache[{layer_idx}] å‹ç¼©åå®é™…é•¿åº¦ = {actual_len}"
                    )
        except Exception as e:
            if self.compression_count <= 3:
                print(f"    âš ï¸ ä¿®æ”¹å¤±è´¥: {e}")

        return args, kwargs

    def register(self, model):
        print("\n[3/6] æ³¨å†Œ Hook...")
        self.remove()

        # æ‰¾åˆ° attention å±‚
        if hasattr(model, "gpt_neox"):
            layers = model.gpt_neox.layers
        elif hasattr(model, "model"):
            layers = model.model.layers
        else:
            print("âŒ æ— æ³•æ‰¾åˆ°æ¨¡å‹çš„ layers")
            return

        print(f"  å‘ç° {len(layers)} ä¸ª Transformer å±‚")

        for i, layer in enumerate(layers):
            if hasattr(layer, "attention"):
                target = layer.attention
                attr_name = "attention"
            elif hasattr(layer, "self_attn"):
                target = layer.self_attn
                attr_name = "self_attn"
            else:
                print(f"  âš ï¸  Layer {i}: æ‰¾ä¸åˆ° attention æ¨¡å—")
                continue

            # ä¸ºæ¯ä¸ªå±‚åˆ›å»ºä¸“å±çš„ hookï¼ˆå¸¦ layer_idxï¼‰
            handle = target.register_forward_pre_hook(
                self._make_hook(i), with_kwargs=True
            )
            self.hooks.append(handle)
            if i == 0:  # åªæ‰“å°ç¬¬ä¸€å±‚
                print(f"  âœ“ Layer 0: æˆåŠŸæ³¨å†Œ Pre-Hook (with_kwargs) åˆ° {attr_name}")

        print(f"âœ“ å…±æ³¨å†Œ {len(self.hooks)} ä¸ª Hook")

    def remove(self):
        for h in self.hooks:
            h.remove()
        self.hooks = []

    def reset_stats(self):
        self.hook_call_count = 0
        self.compression_count = 0


# ========== æµ‹è¯• 1: Baseline (æ—  Press) ==========
print("\n" + "=" * 60)
print("æµ‹è¯• 1: Baseline (æ— å‹ç¼©)")
print("=" * 60)

torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats(DEVICE)

print("\n[4/6] å¼€å§‹ç”Ÿæˆ (Baseline)...")
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,  # å¢åŠ ç”Ÿæˆé•¿åº¦ä»¥è§¦å‘å‹ç¼©
        use_cache=True,
        pad_token_id=tokenizer.eos_token_id,
    )

baseline_mem = torch.cuda.max_memory_allocated(DEVICE) / (1024**2)
baseline_tokens = outputs.shape[1]

print(f"\nâœ“ Baseline å®Œæˆ:")
print(f"  - ç”Ÿæˆ tokens: {baseline_tokens}")
print(f"  - æ˜¾å­˜å³°å€¼: {baseline_mem:.2f} MB")


# ========== æµ‹è¯• 2: æ‰‹åŠ¨å¾ªç¯ + Press (å…³é”®æµ‹è¯•) ==========
print("\n" + "=" * 60)
print("æµ‹è¯• 2: æ‰‹åŠ¨å¾ªç¯ + StreamingLLM Press")
print("=" * 60)

press_manual = DebugPress(max_capacity=3, n_sink=1)
press_manual.register(model)

torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats(DEVICE)

print("\n[4.5/6] å¼€å§‹æ‰‹åŠ¨ç”Ÿæˆå¾ªç¯...")
print("  (ç”Ÿæˆ 500 tokens ä»¥æµ‹è¯•é•¿åºåˆ—å‹ç¼©æ•ˆæœ)\n")

with torch.no_grad():
    input_ids = inputs.input_ids
    past_key_values = None
    generated_tokens = 0

    for step in range(500):
        # å‡†å¤‡è¾“å…¥
        model_inputs = {
            "input_ids": input_ids,
            "use_cache": True,
        }
        if past_key_values is not None:
            model_inputs["past_key_values"] = past_key_values

        # è°ƒç”¨æ¨¡å‹
        outputs = model(**model_inputs)

        # è·å–ä¸‹ä¸€ä¸ª token
        next_token = torch.argmax(outputs.logits[:, -1, :], dim=-1, keepdim=True)

        # å…³é”®ï¼šä¿å­˜æ¨¡å‹è¿”å›çš„ past_key_values
        past_key_values = outputs.past_key_values

        # å‡†å¤‡ä¸‹ä¸€æ¬¡è¾“å…¥
        input_ids = next_token
        generated_tokens += 1

        # æ‰“å°æ¯ä¸€æ­¥çš„ KV cache é•¿åº¦ï¼ˆå‰10æ­¥å’Œå5æ­¥ï¼‰
        if step < 10 or step >= 495:
            if past_key_values is not None:
                kv_len = past_key_values[0][0].shape[2]
                print(f"  Step {step+1}: KV Cache é•¿åº¦ = {kv_len}")

press_manual.remove()

manual_mem = torch.cuda.max_memory_allocated(DEVICE) / (1024**2)

print(f"\nâœ“ æ‰‹åŠ¨å¾ªç¯å®Œæˆ:")
print(f"  - ç”Ÿæˆ tokens: {generated_tokens}")
print(f"  - æ˜¾å­˜å³°å€¼: {manual_mem:.2f} MB")
print(f"  - Hook è°ƒç”¨æ¬¡æ•°: {press_manual.hook_call_count}")
print(f"  - å®é™…å‹ç¼©æ¬¡æ•°: {press_manual.compression_count}")


# ========== æµ‹è¯• 3: generate() + Press (å¯¹æ¯”) ==========
print("\n" + "=" * 60)
print("æµ‹è¯• 3: generate() + Press (å¯¹æ¯”)")
print("=" * 60)

press = DebugPress(max_capacity=3, n_sink=1)
press.reset_stats()
press.register(model)

torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats(DEVICE)

print("\n[5/6] å¼€å§‹ç”Ÿæˆ (generate + Press)...")
print("  (è¿™æ¬¡ä¼šçœ‹åˆ° past_kv å§‹ç»ˆä¸º None)\n")

with torch.no_grad():
    outputs_stream = model.generate(
        **inputs,
        max_new_tokens=100,
        use_cache=True,
        pad_token_id=tokenizer.eos_token_id,
    )

press.remove()

stream_mem = torch.cuda.max_memory_allocated(DEVICE) / (1024**2)
stream_tokens = outputs_stream.shape[1]

print(f"\nâœ“ StreamingLLM å®Œæˆ:")
print(f"  - ç”Ÿæˆ tokens: {stream_tokens}")
print(f"  - æ˜¾å­˜å³°å€¼: {stream_mem:.2f} MB")
print(f"  - Hook è°ƒç”¨æ¬¡æ•°: {press.hook_call_count}")
print(f"  - å®é™…å‹ç¼©æ¬¡æ•°: {press.compression_count}")


# ========== ç»“æœå¯¹æ¯” ==========
print("\n" + "=" * 60)
print("[6/6] ç»“æœå¯¹æ¯”")
print("=" * 60)
print(f"{'æŒ‡æ ‡':<20} | {'Baseline':<12} | {'æ‰‹åŠ¨+Press':<12} | {'generate+Press':<12}")
print("-" * 80)
print(
    f"{'æ˜¾å­˜ (MB)':<20} | {baseline_mem:<12.2f} | {manual_mem:<12.2f} | {stream_mem:<12.2f}"
)
print(
    f"{'Hook å‹ç¼©æ¬¡æ•°':<20} | {'-':<12} | {press_manual.compression_count:<12} | {press.compression_count:<12}"
)

manual_saved = baseline_mem - manual_mem

print("\n" + "=" * 60)
print("ğŸ“Š è¯Šæ–­ç»“æœ")
print("=" * 60)

if press_manual.compression_count > 0 and manual_saved > 0.1:
    print(
        f"âœ… æˆåŠŸï¼æ‰‹åŠ¨å¾ªç¯ + Press æ˜¾å­˜èŠ‚çœäº† {manual_saved:.2f} MB ({manual_saved/baseline_mem*100:.1f}%)"
    )
    print(f"   å®é™…å‹ç¼©äº† {press_manual.compression_count} æ¬¡")
    print("\nğŸ¯ ç»“è®º: StreamingLLM Press å®ç°**æ­£ç¡®**ï¼")
    print("   é—®é¢˜åœ¨äº model.generate() ä¸ä¼ é€’ past_key_values")
    print("\nâœ¨ è§£å†³æ–¹æ¡ˆ:")
    print("   ä½¿ç”¨æ‰‹åŠ¨å¾ªç¯ä»£æ›¿ generate()ï¼ˆå‚è€ƒ benchmark_streaming_manual.pyï¼‰")
elif press_manual.compression_count > 0:
    print(
        f"âš ï¸  æ‰‹åŠ¨å¾ªç¯è§¦å‘äº† {press_manual.compression_count} æ¬¡å‹ç¼©ï¼Œä½†æ˜¾å­˜æœªæ˜æ˜¾ä¸‹é™"
    )
    print("\nå¯èƒ½åŸå› :")
    print("  1. max_capacity=10 è¿˜æ˜¯å¤ªå¤§ï¼Œæ”¹ä¸º 5 è¯•è¯•")
    print("  2. ç”Ÿæˆçš„ token æ•°é‡å¤ªå°‘ï¼ˆåªæœ‰ 50ï¼‰")
    print("  3. æ¨¡å‹çš„å…¶ä»–éƒ¨åˆ†å ç”¨äº†ä¸»è¦æ˜¾å­˜")
else:
    print("âŒ æ‰‹åŠ¨å¾ªç¯ä¹Ÿæ²¡æœ‰è§¦å‘å‹ç¼©")
    print("\nå¯èƒ½åŸå› :")
    print("  1. KV Cache é•¿åº¦å§‹ç»ˆæ²¡è¶…è¿‡ max_capacity=10")
    print("  2. Hook çš„å‹ç¼©é€»è¾‘æœ‰é—®é¢˜")

if press.compression_count == 0 and press.hook_call_count > 0:
    print(f"\nğŸ“Œ generate() é—®é¢˜ç¡®è®¤:")
    print(f"   - Hook è¢«è°ƒç”¨äº† {press.hook_call_count} æ¬¡")
    print(f"   - ä½† past_kv å§‹ç»ˆä¸º None")
    print(f"   - è¿™è¯å®äº† generate() **ä¸ä½¿ç”¨** past_key_values å‚æ•°ä¼ é€’ Cache")
    print(f"   - å¿…é¡»ä½¿ç”¨æ‰‹åŠ¨å¾ªç¯æ‰èƒ½è®© Press ç”Ÿæ•ˆ")

print("\n" + "=" * 60)
