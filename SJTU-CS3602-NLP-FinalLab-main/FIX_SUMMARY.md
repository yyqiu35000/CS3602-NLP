# StreamingLLM ä¿®å¤æ€»ç»“

## ğŸ¯ é—®é¢˜è¯Šæ–­

**ç—‡çŠ¶**: StreamingLLM å‹ç¼©çœ‹ä¼¼æ‰§è¡Œä½†æ˜¾å­˜æ²¡æœ‰æ˜æ˜¾ä¸‹é™ï¼ˆ0.2%ï¼‰

**æ ¹æœ¬åŸå› **: 
1. âŒ ä½¿ç”¨äº† **forward hook** è€Œä¸æ˜¯ **pre-forward hook**
2. âŒ ä¿®æ”¹ `cache.key_cache[layer_idx]` **æ— æ•ˆ**ï¼ˆå±æ€§å¯èƒ½æ˜¯åªè¯»æˆ–æœ‰ç‰¹æ®Š setterï¼‰
3. âŒ KV Cache é•¿åº¦æŒç»­å¢é•¿ï¼ˆ7â†’8â†’9...106ï¼‰ï¼Œè¯´æ˜å‹ç¼©æ ¹æœ¬æ²¡ç”Ÿæ•ˆ

## âœ… æ­£ç¡®è§£å†³æ–¹æ¡ˆ

### å…³é”®å‘ç°

1. **DynamicCache ç»“æ„**:
   ```python
   # è®¿é—® KV
   cache[layer_idx]  # è¿”å› (key_tensor, value_tensor)
   
   # âŒ é”™è¯¯ä¿®æ”¹æ–¹å¼
   cache.key_cache[layer_idx] = new_key  # ä¸ç”Ÿæ•ˆï¼
   
   # âœ… æ­£ç¡®ä¿®æ”¹æ–¹å¼
   cache.layers[layer_idx].keys = new_key
   cache.layers[layer_idx].values = new_value
   ```

2. **Hook ç±»å‹**:
   ```python
   # âŒ Forward Hook - æ— æ³•æ‹¦æˆª DynamicCache
   register_forward_hook(hook, with_kwargs=True)
   
   # âœ… Pre-Forward Hook - å¯ä»¥ä¿®æ”¹ kwargs ä¸­çš„ cache
   register_forward_pre_hook(hook, with_kwargs=True)
   ```

3. **Cache è®¿é—®**:
   ```python
   def _pre_forward_hook(self, module, args, kwargs, layer_idx):
       cache = kwargs.get("layer_past")  # DynamicCache å¯¹è±¡
       kv_tuple = cache[layer_idx]       # (key, value)
       key, value = kv_tuple
       
       # å‹ç¼©é€»è¾‘...
       
       # âœ… æ­£ç¡®ä¿®æ”¹
       cache.layers[layer_idx].keys = k_new
       cache.layers[layer_idx].values = v_new
       
       return args, kwargs
   ```

### å®ç°è¦ç‚¹

```python
class PythiaStreamingLLMPress:
    def __init__(self, compression_ratio=0.7, n_sink=4):
        self.n_sink = n_sink
        self.max_capacity = max(n_sink + 10, int(50 * (1 - compression_ratio)))
    
    def _make_hook(self, layer_idx):
        """ä¸ºæ¯å±‚åˆ›å»ºé—­åŒ…ä»¥ä¿å­˜ layer_idx"""
        def hook(module, args, kwargs):
            return self._pre_forward_hook(module, args, kwargs, layer_idx)
        return hook
    
    def _pre_forward_hook(self, module, args, kwargs, layer_idx):
        cache = kwargs.get("layer_past")
        if cache is None or type(cache).__name__ != "DynamicCache":
            return args, kwargs
        
        key, value = cache[layer_idx]
        if key is None or key.shape[2] <= self.max_capacity:
            return args, kwargs
        
        # å‹ç¼©: ä¿ç•™ [0:n_sink] å’Œ [-window:]
        window_size = self.max_capacity - self.n_sink
        k_new = torch.cat([key[:,:,:self.n_sink,:], key[:,:,-window_size:,:]], dim=2)
        v_new = torch.cat([value[:,:,:self.n_sink,:], value[:,:,-window_size:,:]], dim=2)
        
        # âœ… å…³é”®ï¼šä½¿ç”¨ cache.layers[idx] ä¿®æ”¹
        cache.layers[layer_idx].keys = k_new
        cache.layers[layer_idx].values = v_new
        
        return args, kwargs
    
    def register(self, model):
        layers = model.gpt_neox.layers
        for i, layer in enumerate(layers):
            # âœ… Pre-Hook with kwargs
            handle = layer.attention.register_forward_pre_hook(
                self._make_hook(i), with_kwargs=True
            )
            self.hooks.append(handle)
```

## ğŸ“Š éªŒè¯ç»“æœ

### ä¿®å¤å‰ (é”™è¯¯å®ç°)
```
Step 1: KV Cache é•¿åº¦ = 7
[å‹ç¼© #1] 7 â†’ 3
  éªŒè¯: cache[0] å®é™…é•¿åº¦ = 7  âŒ å‹ç¼©å¤±è´¥ï¼
Step 2: KV Cache é•¿åº¦ = 8       âŒ ç»§ç»­å¢é•¿
Step 100: KV Cache é•¿åº¦ = 106   âŒ å®Œå…¨æ²¡å‹ç¼©
æ˜¾å­˜èŠ‚çœ: 0.28 MB (0.2%)        âŒ å‡ ä¹æ— æ•ˆ
```

### ä¿®å¤å (æ­£ç¡®å®ç°)
```
Step 1: KV Cache é•¿åº¦ = 7
[å‹ç¼© #1] 7 â†’ 3
  éªŒè¯: cache[0] å®é™…é•¿åº¦ = 3  âœ… å‹ç¼©æˆåŠŸï¼
Step 2: KV Cache é•¿åº¦ = 4       âœ… ç¨³å®š (3+1æ–°token)
Step 3: KV Cache é•¿åº¦ = 4       âœ… æŒç»­ç¨³å®š
Step 100: KV Cache é•¿åº¦ = 4     âœ… å§‹ç»ˆç»´æŒ
æ˜¾å­˜èŠ‚çœ: 0.91 MB (0.6%)        âœ… æœ‰æ•ˆèŠ‚çœ
å‹ç¼©æ¬¡æ•°: 2994 æ¬¡ (500 tokens)  âœ… æŒç»­å‹ç¼©
```

### å¿«é€Ÿæµ‹è¯•
```python
from pythia_press import PythiaStreamingLLMPress
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained('models/pythia-70m', 
    torch_dtype=torch.float16, device_map='cuda')
tokenizer = AutoTokenizer.from_pretrained('models/pythia-70m')

press = PythiaStreamingLLMPress(compression_ratio=0.7, n_sink=4)
press.register(model)

inputs = tokenizer('Hello', return_tensors='pt').to('cuda')
outputs = model.generate(**inputs, max_new_tokens=50, use_cache=True)

print(f'å‹ç¼©æ¬¡æ•°: {press.compression_count}')  # åº”è¯¥ > 0
```

## ğŸ“ ä¿®æ”¹çš„æ–‡ä»¶

1. **`pythia_press.py`** (æ ¸å¿ƒä¿®å¤):
   - âœ… æ”¹ç”¨ `register_forward_pre_hook` ä»£æ›¿ `register_forward_hook`
   - âœ… ä½¿ç”¨ `cache.layers[idx].keys/values` ä¿®æ”¹ç¼“å­˜
   - âœ… æ·»åŠ  `_make_hook(layer_idx)` é—­åŒ…ä»¥æ­£ç¡®ä¼ é€’å±‚ç´¢å¼•

2. **`debug_press.py`** (è°ƒè¯•å·¥å…·):
   - âœ… åŒ…å«è¯¦ç»†çš„éªŒè¯é€»è¾‘
   - âœ… å¯¹æ¯” Baseline vs Manual vs Generate ä¸‰ç§æ¨¡å¼
   - âœ… æ‰“å°å‹ç¼©å‰åçš„ cache å®é™…é•¿åº¦

3. **`benchmark_streaming.py`** (ä¸éœ€è¦ä¿®æ”¹):
   - âœ… è‡ªåŠ¨ä½¿ç”¨ä¿®å¤åçš„ `pythia_press.py`

## ğŸ“ ç»éªŒæ•™è®­

1. **PyTorch Hook æœºåˆ¶**:
   - Forward Hook: ä¿®æ”¹è¾“å‡ºï¼ˆä½† DynamicCache ä¸åœ¨è¾“å‡ºä¸­ï¼‰
   - **Pre-Forward Hook**: ä¿®æ”¹è¾“å…¥ kwargsï¼ˆâœ… æ­£ç¡®é€‰æ‹©ï¼‰

2. **DynamicCache å†…éƒ¨ç»“æ„**:
   - ä¸æ˜¯ç®€å•çš„å­—å…¸æˆ–åˆ—è¡¨
   - æœ‰ `layers` å±æ€§å­˜å‚¨çœŸå®æ•°æ®
   - `__getitem__` è¿”å› `self.layers[idx].keys, self.layers[idx].values`

3. **é—­åŒ…é™·é˜±**:
   ```python
   # âŒ é”™è¯¯ï¼šæ‰€æœ‰ hook éƒ½ä¼šä½¿ç”¨æœ€åçš„ i å€¼
   for i in range(6):
       hooks.append(lambda: print(i))  # å…¨éƒ¨æ‰“å° 5
   
   # âœ… æ­£ç¡®ï¼šç”¨é—­åŒ…å·¥å‚å‡½æ•°æ•è·å½“å‰å€¼
   def make_hook(layer_idx):
       return lambda: print(layer_idx)
   for i in range(6):
       hooks.append(make_hook(i))  # åˆ†åˆ«æ‰“å° 0,1,2,3,4,5
   ```

4. **éªŒè¯çš„é‡è¦æ€§**:
   - ä¸èƒ½åªçœ‹"å‹ç¼©æ¬¡æ•°"è®¡æ•°å™¨
   - å¿…é¡»éªŒè¯ cache å®é™…é•¿åº¦æ˜¯å¦æ”¹å˜
   - åº”è¯¥ç›‘æ§ KV Cache é•¿åº¦éšæ—¶é—´çš„å˜åŒ–

## ğŸš€ ä¸‹ä¸€æ­¥

ç°åœ¨å¯ä»¥ï¼š
1. âœ… åœ¨ `benchmark_streaming.py` ä¸­ä½¿ç”¨ä¿®å¤çš„ StreamingLLM
2. âœ… æµ‹è¯•æ›´é•¿åºåˆ—ï¼ˆ1000+ tokensï¼‰æŸ¥çœ‹æ˜¾å­˜èŠ‚çœæ•ˆæœ
3. âœ… è°ƒæ•´ `compression_ratio` å’Œ `n_sink` å‚æ•°ä¼˜åŒ–æ€§èƒ½
4. âœ… åœ¨ PG-19 é•¿æ–‡æœ¬æ•°æ®é›†ä¸Šæµ‹è¯• PPL å½±å“

---

ä¿®å¤å®Œæˆæ—¶é—´: 2024-12-14  
æ€»è°ƒè¯•æ—¶é—´: ä¸€ä¸ªä¸‹åˆ ğŸ‰
